{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, random, tarfile\n",
    "import sys, argparse\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py as h5\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from skimage import io, transform\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "import torch, torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch.utils.data as data\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision.datasets.utils import check_integrity, download_url\n",
    "import torchvision.transforms as transforms\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "import time\n",
    "\n",
    "from scipy.io import loadmat # for loading .mat files\n",
    "#from scipy.io import savemat # for saving .mat files\n",
    "\n",
    "from sklearn.metrics.cluster import normalized_mutual_info_score#, homogeneity_score\n",
    "import sklearn\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "#######\n",
    "\n",
    "import pymanopt\n",
    "from pymanopt.manifolds import Product, Euclidean,  Grassmann\n",
    "from pymanopt import Problem\n",
    "from pymanopt.solvers import ConjugateGradient#, SteepestDescent\n",
    "\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    meaningless_dummy_variable=999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Following parameters are used=> emb_dim:  128  n_epochs:  20  batch_sz:  120\n"
     ]
    }
   ],
   "source": [
    "dataset_name='CUB'#CUB,Cars,SOP\n",
    "loss_name='gumlLoss'\n",
    "\n",
    "# ### Uncomment the below for command-line arguments\n",
    "# parser = argparse.ArgumentParser()\n",
    "# LookupChoices = type('', (argparse.Action, ), dict(__call__ = lambda a, p, n, v, o: setattr(n, a.dest, a.choices[v])))\n",
    "# parser.add_argument('--emb_dim', default = 128, type = int)\n",
    "# parser.add_argument('--epochs', default = 5, type = int)\n",
    "# parser.add_argument('--batch', default = 42, type = int)\n",
    "# parser.add_argument('--lr', default = 0.01, type = float)\n",
    "# parser.add_argument('--margin', default = 0.5, type = float)\n",
    "# parser.add_argument('--step-size', default = 50, type = int)\n",
    "# parser.add_argument('--gamma', default = 0.1, type = float)\n",
    "# #parser.add_argument('--gpu-id', default='0', type=str)\n",
    "# opts = parser.parse_args()\n",
    "\n",
    "\n",
    "opts=Args()\n",
    "\n",
    "# Loss term related hyperparameters\n",
    "opts.margin = 0.5 # for triplet loss, and semi-hard mining\n",
    "opts.alpha = 45 # for guml loss\n",
    "\n",
    "# Embedding size\n",
    "opts.emb_dim = 128\n",
    "\n",
    "# Optimization related hyperparameters\n",
    "opts.epochs = 20\n",
    "opts.batch = 120\n",
    "\n",
    "opts.lr = 0.01\n",
    "opts.step_size = 50\n",
    "opts.gamma = 0.1\n",
    "\n",
    "print('Following parameters are used=> emb_dim: ',opts.emb_dim,' n_epochs: ',opts.epochs,' batch_sz: ',opts.batch)\n",
    "\n",
    "emb_dim = opts.emb_dim\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "#print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _check_integrity(img_folder, integrity_test_list):\n",
    "    for fentry in (integrity_test_list):\n",
    "        filename, md5 = fentry[0], fentry[1]\n",
    "        fpath = img_folder + filename\n",
    "        if not check_integrity(fpath, md5):\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pil_loader(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        img = Image.open(f)\n",
    "        return img.convert('RGB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_and_extract_archive(url, download_root, extract_root=None, filename=None,\n",
    "                                 md5=None, remove_finished=False):\n",
    "    download_root = os.path.expanduser(download_root)\n",
    "    if extract_root is None:\n",
    "        extract_root = download_root\n",
    "    if not filename:\n",
    "        filename = os.path.basename(url)\n",
    "\n",
    "    download_url(url, download_root, filename, md5)\n",
    "\n",
    "    archive = os.path.join(download_root, filename)\n",
    "    print(\"Extracting {} to {}\".format(archive, extract_root))\n",
    "    \n",
    "    with tarfile.open(archive, 'r:gz') as tar: tar.extractall(path=extract_root)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================================================\n",
    "class CUB2011():\n",
    "    root = '../data/'\n",
    "    url = 'http://www.vision.caltech.edu/visipedia-data/CUB-200-2011/CUB_200_2011.tgz'\n",
    "    filename = 'CUB_200_2011.tgz'\n",
    "    tgz_md5 = '97eceeb196236b17998738112f37df78'\n",
    "\n",
    "    num_training_classes = 100\n",
    "    name = 'CUB_200_2011'\n",
    "    triplet_mode = False\n",
    "    mined_data = None\n",
    "\n",
    "    integrity_test_list = [\n",
    "    ['001.Black_footed_Albatross/Black_Footed_Albatross_0001_796111.jpg', '4c84da568f89519f84640c54b7fba7c2'],\n",
    "    ['002.Laysan_Albatross/Laysan_Albatross_0001_545.jpg', 'e7db63424d0e384dba02aacaf298cdc0'],\n",
    "    ['198.Rock_Wren/Rock_Wren_0001_189289.jpg', '487d082f1fbd58faa7b08aa5ede3cc00'],\n",
    "    ['200.Common_Yellowthroat/Common_Yellowthroat_0003_190521.jpg', '96fd60ce4b4805e64368efc32bf5c6fe']\n",
    "    ]\n",
    "\n",
    "\n",
    "    def __init__(self, transform=None, download=False, train = True, **kwargs):\n",
    "        if download and not _check_integrity(self.root+'/CUB_200_2011/images/', self.integrity_test_list):\n",
    "            download_and_extract_archive(self.url, self.root, filename=self.filename, md5=self.tgz_md5)\n",
    "\n",
    "        if not _check_integrity(self.root+'/CUB_200_2011/images/', self.integrity_test_list):\n",
    "            raise RuntimeError('Dataset not found or corrupted. You can use download=True to download it')\n",
    "        else:\n",
    "            print('Dataset found, and is proper!') \n",
    "\n",
    "        self.transform = transform\n",
    "        self.train = train\n",
    "\n",
    "        self.classes = [x.split()[-1] for x in open(self.root+self.name+\"/classes.txt\", \"r\").readlines()]\n",
    "        self.class_to_idx = {_class: i for i, _class in enumerate(self.classes)}\n",
    "        self.classes = self.classes[:self.num_training_classes] if train else self.classes[self.num_training_classes:]\n",
    "\n",
    "        self.idx_to_class = {v: k for k, v in self.class_to_idx.items()}\n",
    "        \n",
    "        \n",
    "        i = open(self.root+self.name+\"/images.txt\", \"r\").readlines() \n",
    "        # <image_id,image_file_name>: image_id (example number in dataset) #image_id:1,2,...N\n",
    "        l = open(self.root+self.name+\"/image_class_labels.txt\", \"r\").readlines()\n",
    "        # <image_id,class_label> #print('l:\\n',l) #class_label:1,2,...C\n",
    "        self.imgs = [(_[0].split()[1], int(_[1].split()[1])-1) for _ in zip(i,l)]\n",
    "        # <image_file_name,class_label>: class_label:0,1,... due to -1, for python based indexing\n",
    "\n",
    "        \n",
    "        self.imgs = [(self.root+self.name+\"/images/\"+image_file_path, class_label_ind) for image_file_path, class_label_ind in self.imgs if ((class_label_ind-self.num_training_classes)<0) == self.train ]\n",
    "        #<exact_train_image_path,class_label> // only for the training data        \n",
    "        \n",
    "        self.loader = pil_loader\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        # triplet (anchor, pos, neg) mode\n",
    "        if self.triplet_mode:\n",
    "            perm = random.sample(self.mined_data['anchors'].keys(), 1)[0] # pick a random anchor\n",
    "\n",
    "            # sort negatives based on Euclidean distance to anchor\n",
    "            q_x = self.embeddings[perm,:]\n",
    "            n_x = self.embeddings[self.mined_data['negpool'][perm],:] # get the embeddings of the list of negatives for the selected anchor\n",
    "            dists = ((q_x - n_x)**2).sum(axis=1) # broadcasting\n",
    "            n_idx = np.argsort(dists) \n",
    "\n",
    "            rand_pos = np.random.randint(0, len(self.mined_data['pospool'][perm])) # random pos from the pool\n",
    "            rand_neg = n_idx[np.random.randint(0, np.min((10, len(self.mined_data['negpool'][perm]))))]  # random pick from 10 Euclidean-NN in the pool\n",
    "\n",
    "            a_path, a_target = self.imgs[int(self.mined_data['anchors'][perm])] # now select the actual anchor image\n",
    "            p_path, p_target = self.imgs[int(self.mined_data['pospool'][perm][rand_pos])]\n",
    "            n_path, n_target = self.imgs[int(self.mined_data['negpool'][perm][rand_neg])]\n",
    "\n",
    "#             p_w = self.mined_data['posweight'][perm][rand_pos] #wts not needed to be given as input\n",
    "#             n_w = self.mined_data['negweight'][perm][rand_neg]\n",
    "\n",
    "            a_img, p_img, n_img = self.loader(a_path), self.loader(p_path), self.loader(n_path) \n",
    "\n",
    "            if self.transform is not None:\n",
    "                a_img, p_img, n_img = self.transform(a_img), self.transform(p_img), self.transform(n_img)\n",
    "\n",
    "            return a_img, p_img, n_img#, p_w, n_w\n",
    "\n",
    "        # single image mode\n",
    "        else:\n",
    "            path, target = self.imgs[index] # if index is given, one get directly get the (image,label)-tuple\n",
    "            img = self.loader(path)\n",
    "            if self.transform is not None:\n",
    "                img = self.transform(img)\n",
    "\n",
    "            return img, target\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class inception_v1_googlenet(nn.Sequential):\n",
    "    output_size = 1024\n",
    "    input_side = 227\n",
    "    rescale = 255.0\n",
    "    rgb_mean = [122.7717, 115.9465, 102.9801]\n",
    "    rgb_std = [1, 1, 1]\n",
    "\n",
    "    def __init__(self):\n",
    "        super(inception_v1_googlenet, self).__init__(OrderedDict([\n",
    "            ('conv1', nn.Sequential(OrderedDict([\n",
    "            ('7x7_s2', nn.Conv2d(3, 64, (7, 7), (2, 2), (3, 3))),\n",
    "            ('relu1', nn.ReLU(True)),\n",
    "            ('pool1', nn.MaxPool2d((3, 3), (2, 2), ceil_mode = True)),\n",
    "            ('lrn1', nn.CrossMapLRN2d(5, 0.0001, 0.75, 1))\n",
    "            ]))),\n",
    "\n",
    "            ('conv2', nn.Sequential(OrderedDict([\n",
    "            ('3x3_reduce', nn.Conv2d(64, 64, (1, 1), (1, 1), (0, 0))),\n",
    "            ('relu1', nn.ReLU(True)),\n",
    "            ('3x3', nn.Conv2d(64, 192, (3, 3), (1, 1), (1, 1))),\n",
    "            ('relu2', nn.ReLU(True)),\n",
    "            ('lrn2', nn.CrossMapLRN2d(5, 0.0001, 0.75, 1)),\n",
    "            ('pool2', nn.MaxPool2d((3, 3), (2, 2), ceil_mode = True))\n",
    "            ]))),\n",
    "\n",
    "            ('inception_3a', InceptionModule(192, 64, 96, 128, 16, 32, 32)),\n",
    "            ('inception_3b', InceptionModule(256, 128, 128, 192, 32, 96, 64)),\n",
    "\n",
    "            ('pool3', nn.MaxPool2d((3, 3), (2, 2), ceil_mode = True)),\n",
    "\n",
    "            ('inception_4a', InceptionModule(480, 192, 96, 208, 16, 48, 64)),\n",
    "            ('inception_4b', InceptionModule(512, 160, 112, 224, 24, 64, 64)),\n",
    "            ('inception_4c', InceptionModule(512, 128, 128, 256, 24, 64, 64)),\n",
    "            ('inception_4d', InceptionModule(512, 112, 144, 288, 32, 64, 64)),\n",
    "            ('inception_4e', InceptionModule(528, 256, 160, 320, 32, 128, 128)),\n",
    "\n",
    "            ('pool4', nn.MaxPool2d((3, 3), (2, 2), ceil_mode = True)),\n",
    "\n",
    "            ('inception_5a', InceptionModule(832, 256, 160, 320, 32, 128, 128)),\n",
    "            ('inception_5b', InceptionModule(832, 384, 192, 384, 48, 128, 128)),\n",
    "\n",
    "            ('pool5', nn.AvgPool2d((7, 7), (1, 1), ceil_mode = True)),\n",
    "\n",
    "            #('drop5', nn.Dropout(0.4))\n",
    "            ]))\n",
    "\n",
    "class InceptionModule(nn.Module):\n",
    "    def __init__(self, inplane, outplane_a1x1, outplane_b3x3_reduce, outplane_b3x3, outplane_c5x5_reduce, outplane_c5x5, outplane_pool_proj):\n",
    "        super(InceptionModule, self).__init__()\n",
    "        a = nn.Sequential(OrderedDict([\n",
    "            ('1x1', nn.Conv2d(inplane, outplane_a1x1, (1, 1), (1, 1), (0, 0))),\n",
    "            ('1x1_relu', nn.ReLU(True))\n",
    "            ]))\n",
    "\n",
    "        b = nn.Sequential(OrderedDict([\n",
    "            ('3x3_reduce', nn.Conv2d(inplane, outplane_b3x3_reduce, (1, 1), (1, 1), (0, 0))),\n",
    "            ('3x3_relu1', nn.ReLU(True)),\n",
    "            ('3x3', nn.Conv2d(outplane_b3x3_reduce, outplane_b3x3, (3, 3), (1, 1), (1, 1))),\n",
    "            ('3x3_relu2', nn.ReLU(True))\n",
    "            ]))\n",
    "\n",
    "        c = nn.Sequential(OrderedDict([\n",
    "            ('5x5_reduce', nn.Conv2d(inplane, outplane_c5x5_reduce, (1, 1), (1, 1), (0, 0))),\n",
    "            ('5x5_relu1', nn.ReLU(True)),\n",
    "            ('5x5', nn.Conv2d(outplane_c5x5_reduce, outplane_c5x5, (5, 5), (1, 1), (2, 2))),\n",
    "            ('5x5_relu2', nn.ReLU(True))\n",
    "            ]))\n",
    "\n",
    "        d = nn.Sequential(OrderedDict([\n",
    "            ('pool_pool', nn.MaxPool2d((3, 3), (1, 1), (1, 1))),\n",
    "            ('pool_proj', nn.Conv2d(inplane, outplane_pool_proj, (1, 1), (1, 1), (0, 0))),\n",
    "            ('pool_relu', nn.ReLU(True))\n",
    "            ]))\n",
    "\n",
    "        for container in [a, b, c, d]:\n",
    "            for name, module in container.named_children():\n",
    "                self.add_module(name, module)\n",
    "\n",
    "        self.branches = [a, b, c, d]\n",
    "\n",
    "    def forward(self, input):\n",
    "        return torch.cat([branch(input) for branch in self.branches], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def distance_vectors_pairwise(anchor, positive, negative , squared = True):\n",
    "#     \"\"\"Given batch of anchor descriptors and positive descriptors calculate distance matrix\"\"\"\n",
    "#     eps = 1e-8\n",
    "\n",
    "#     a_sq = torch.sum(anchor * anchor, dim=1)\n",
    "#     p_sq = torch.sum(positive * positive, dim=1)\n",
    "#     n_sq = torch.sum(negative * negative, dim=1)\n",
    "\n",
    "#     d_a_p = a_sq + p_sq - 2*torch.sum(anchor * positive, dim = 1)\n",
    "#     d_a_n = a_sq + n_sq - 2*torch.sum(anchor * negative, dim = 1)\n",
    "#     d_p_n = p_sq + n_sq - 2*torch.sum(positive * negative, dim = 1)\n",
    "    \n",
    "#     if not squared:\n",
    "#         d_a_p = torch.sqrt(d_a_p + eps)\n",
    "#         d_a_n = torch.sqrt(d_a_n + eps)\n",
    "#         d_p_n = torch.sqrt(d_p_n + eps)\n",
    "        \n",
    "#     return d_a_p, d_a_n, d_p_n\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNModel(nn.Module):\n",
    "    def __init__(self, base_model, num_classes):#, embedding_size = 128, lr = 0.001):\n",
    "        super(CNNModel, self).__init__()\n",
    "        self.base_model = base_model\n",
    "        self.num_classes = num_classes\n",
    "        #self.embedder = nn.Linear(base_model.output_size, embedding_size)\n",
    "        #self.lr = lr\n",
    "        \n",
    "    def forward(self, input):\n",
    "        #return self.embedder(F.relu(self.base_model(input).view(len(input), -1)))\n",
    "        return F.normalize(F.relu(self.base_model(input).view(len(input), -1)))\n",
    "    \n",
    "    #criterion = None\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripletLoss(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Triplet loss function.\n",
    "    Based on: FaceNet: A unified embedding for face recognition and clustering\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, margin=0.5):\n",
    "        super(TripletLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, output1, output2, output3):\n",
    "        anc_pos_distance = F.pairwise_distance(output1, output2, keepdim = True) # output1:(b,d),output2:(b,d)\n",
    "        anc_neg_distance = F.pairwise_distance(output1, output3, keepdim = True) # output1:(b,d),output2:(b,d)\n",
    "        \n",
    "        #print(euclidean_distance.size()) # (b,1) pairwise distances tensor\n",
    "#         temp=((1-label) * torch.pow(euclidean_distance, 2) +\n",
    "#                                       (label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2))\n",
    "        \n",
    "        #print('temp:',temp.size()) ## (b,1) tensor\n",
    "        loss_triplet = torch.mean(torch.pow(torch.clamp(anc_pos_distance+self.margin - anc_neg_distance, min=0.0), 2))\n",
    "        #print('loss_triplet:',loss_triplet) # tensor of no size, but an item containing the scalar loss \n",
    "\n",
    "        return loss_triplet\n",
    "\n",
    "############################################################################################################\n",
    "# class WeightedTriplet(Model):\n",
    "#     def forward(self, input):\n",
    "#         return F.normalize(Model.forward(self, input))\n",
    "    \n",
    "#     def criterion(self, a_emb, p_emb, n_emb, margin = 1.0):\n",
    "#     #def criterion(self, a_emb, p_emb, n_emb, p_w, n_w, margin = 1.0):\n",
    "#         (d_a_p, d_a_n, _) = distance_vectors_pairwise(a_emb, p_emb, n_emb)\n",
    "#         loss = torch.clamp(margin + d_a_p - d_a_n, min=0.0)\n",
    "#         #loss = loss * p_w.float()# no need to give weights as i/p, can be computed\n",
    "#         loss = torch.mean(loss)\n",
    "\n",
    "#         return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class gumlLoss(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    GUML loss function.\n",
    "    Based on: \n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, alpha=45):\n",
    "        super(gumlLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def forward(self, embed1, embed2, embed3, R_tensor,L_tensor):\n",
    "\n",
    "        xi_ancs=embed1.transpose(0,1)\n",
    "        xi_poss=embed2.transpose(0,1)\n",
    "        xi_negs=embed3.transpose(0,1) #(d,b) format, b:#triplets in mini-batch, i.e., batch size\n",
    "\n",
    "        #print(xi_ancs.size(),xi_poss.size(),xi_negs.size())\n",
    "        #print(xi_ancs.requires_grad,xi_poss.requires_grad,xi_negs.requires_grad)\n",
    "\n",
    "\n",
    "        #print(R_tensor.size(),L_tensor.size())\n",
    "        #print(R_tensor.requires_grad,L_tensor.requires_grad)\n",
    "\n",
    "        xi_avgs=0.5*(xi_ancs+xi_poss) #dxT, T:#triplets\n",
    "        #print(xi_avgs.size())\n",
    "        #print(xi_avgs.requires_grad)\n",
    "\n",
    "        num_triplets=xi_avgs.size(1)\n",
    "        #print(num_triplets)\n",
    "\n",
    "\n",
    "        RRT=R_tensor@(R_tensor.transpose(0,1))\n",
    "        #print(RRT.size(),RRT.dtype)\n",
    "\n",
    "\n",
    "        listA=xi_ancs.transpose(0,1)#.tolist()\n",
    "        listB=xi_poss.transpose(0,1)#.tolist()\n",
    "\n",
    "\n",
    "\n",
    "        #print(listA.dtype,type(listA))\n",
    "\n",
    "        exps_plus=list(map(lambda elA, elB:\n",
    "                   torch.exp(-(elA@RRT@elB)), \n",
    "                   listA.double(), listB.double()))\n",
    "        #exps_plus=list(map(lambda elt: np.asscalar(elt), exps_plus))\n",
    "        exps_plus=torch.tensor(exps_plus).reshape(1,len(exps_plus))\n",
    "\n",
    "        #print(exps_plus,exps_plus.size())\n",
    "\n",
    "        listA=xi_avgs.transpose(0,1)\n",
    "        listB=xi_negs.transpose(0,1)\n",
    "\n",
    "        exps_minus=list(map(lambda elA, elB:\n",
    "                    torch.exp(-(elA@RRT@elB)),\n",
    "                    listA.double(), listB.double()))\n",
    "        #exps_minus=list(map(lambda elt: np.asscalar(elt), exps_minus))\n",
    "        exps_minus=torch.tensor(exps_minus).reshape(1,len(exps_minus))\n",
    "\n",
    "        w_is_plus = 1/(1+ exps_plus ) # weights\n",
    "        w_is_minus = 1-1/(1+ exps_minus ) # weights\n",
    "\n",
    "        w_is=0.5*(w_is_plus+w_is_minus) \n",
    "        #print(w_is,w_is.size())    \n",
    "\n",
    "        LT=L_tensor.transpose(0,1)\n",
    "\n",
    "        delZaZp=xi_ancs-xi_poss\n",
    "        MhZaZp = LT @ delZaZp.double()\n",
    "        d_aps=torch.sum(MhZaZp*MhZaZp,0)\n",
    "        d_aps=d_aps.reshape(1,len(d_aps)) # 1xT\n",
    "        #print(delZaZp.shape,MhZaZp.shape,d_aps.shape)\n",
    "\n",
    "        delZnZm=xi_negs-xi_avgs\n",
    "        MhZnZm = LT @ delZnZm.double()\n",
    "        d_nms=torch.sum(MhZnZm*MhZnZm,0)\n",
    "        d_nms=d_nms.reshape(1,len(d_nms)) # 1xT\n",
    "\n",
    "        tan_sq_alpha=(math.tan(math.radians(self.alpha)))**2 # for angular \n",
    "\n",
    "        z_is=d_aps-4*tan_sq_alpha*d_nms # metric losses\n",
    "\n",
    "        #print(z_is.shape)\n",
    "\n",
    "        m_is=torch.log(1+torch.exp(z_is))\n",
    "        \n",
    "        w_is=w_is.to(device)\n",
    "        m_is=m_is.to(device)\n",
    "\n",
    "        f_is=-w_is*m_is\n",
    "        #print(f_is.shape)\n",
    "\n",
    "        loss_guml= (1/num_triplets)*torch.sum(torch.log(1+torch.exp(-f_is)))\n",
    "\n",
    "        #print(loss_guml)\n",
    "        return loss_guml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_embeddings(model, dataset, threads = 8): \n",
    "    embeddings_all, labels_all = [], []\n",
    "    loader = torch.utils.data.DataLoader(dataset, batch_size=128, shuffle=False, num_workers = threads)\n",
    "    for batch_idx, batch in enumerate(loader):\n",
    "        with torch.no_grad():\n",
    "            images, labels = [torch.autograd.Variable(tensor.to(device)) for tensor in batch]\n",
    "        embeddings_all.extend(model(images).data.cpu().numpy())\n",
    "        labels_all.extend(labels.data.cpu().numpy())\n",
    "    return np.asarray(embeddings_all), np.asarray(labels_all)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nearest Neighbor (NN) search and recall computation\n",
    "def recall(embeddings, labels, K = 1): # embeddings: Nxd\n",
    "    prod = torch.mm(embeddings, embeddings.t())\n",
    "    norm = prod.diag().unsqueeze(1).expand_as(prod)\n",
    "    D = norm + norm.t() - 2 * prod\n",
    "    knn_inds = D.topk(1 + K, dim = 1, largest = False)[1][:, 1:]\n",
    "    \n",
    "#     print(type(embeddings),type(labels),type(prod),type(norm),type(D),type(knn_inds))\n",
    "#     print(embeddings.size(),labels.size(),prod.size(),norm.size(),D.size(),knn_inds.size())\n",
    "    \n",
    "    return (labels.unsqueeze(-1).expand_as(knn_inds) == labels[knn_inds.contiguous().view(-1)].view_as(knn_inds)).max(1)[0].float().mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_nmi(embedding, label,  normed_flag = False, fast_kmeans = False): # provide Nxd data, (N,) labels\n",
    "    unique_id = np.unique(label)\n",
    "    num_category = len(unique_id)\n",
    "    if normed_flag:\n",
    "        for i in range(embedding.shape[0]):\n",
    "            embedding[i,:] = embedding[i,:]/np.sqrt(np.sum(embedding[i,:] ** 2)+1e-4)\n",
    "    if fast_kmeans:\n",
    "        kmeans = KMeans(n_clusters=num_category, n_init = 1)\n",
    "    else:\n",
    "        kmeans = KMeans(n_clusters=num_category)\n",
    "    kmeans.fit(embedding)\n",
    "    y_kmeans_pred = kmeans.predict(embedding)\n",
    "    nmi = normalized_mutual_info_score(label, y_kmeans_pred)\n",
    "    return nmi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load triplet information from text files\n",
    "def read_mined_data(anchors_fn, pos_fn, neg_fn):#, posw_fn, negw_fn):\n",
    "\n",
    "    anchors, pos, neg = dict(), dict(), dict()\n",
    "    #anchors, pos, neg, posw, negw = dict(), dict(), dict(), dict(), dict()\n",
    "    with open(anchors_fn) as f:\n",
    "        for idx,line in enumerate(f):\n",
    "            anchors[idx] = int(line.strip())-1 #stripping to omit \\n, and storing the anc id. -1 for python indexing\n",
    "\n",
    "    with open(pos_fn) as posf, open(neg_fn) as negf:#, open(posw_fn) as poswf, open(negw_fn) as negwf:\n",
    "        #for idx, (pos_line, neg_line, posw_line, negw_line) in enumerate(zip(posf, negf, poswf, negwf)):\n",
    "        for idx, (pos_line, neg_line) in enumerate(zip(posf, negf)):\n",
    "            pos[idx] = [x-1 for x in map(int,pos_line.strip().split(','))]\n",
    "            neg[idx] = [x-1 for x in map(int,neg_line.strip().split(','))]\n",
    "#             posw[idx] = [x for x in map(float,posw_line.strip().split(','))]\n",
    "#             negw[idx] = [x for x in map(float,negw_line.strip().split(','))]\n",
    "\n",
    "    return {'anchors':anchors,'pospool':pos,'negpool':neg}#,'posweight':posw,'negweight':negw}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SemiHardMiner():\n",
    "#     def __init__(self, opt):\n",
    "#         self.par          = opt\n",
    "#         self.name         = 'semihard'\n",
    "#         self.margin       = vars(opt)['loss_'+opt.loss+'_margin']\n",
    "    def __init__(self, opt):\n",
    "        self.margin       = opt.margin\n",
    "\n",
    "    def __call__(self, batch, labels, return_distances=False):\n",
    "        if isinstance(labels, torch.Tensor): labels = labels.detach()#.numpy()\n",
    "        bs = batch.size(0)\n",
    "        #Return distance matrix for all elements in batch (BSxBS)\n",
    "        distances = self.pdist(batch.detach()).detach().cpu().numpy()\n",
    "\n",
    "        positives, negatives = [], []\n",
    "        anchors = []\n",
    "        for i in range(bs):\n",
    "            l, d = labels[i], distances[i]\n",
    "            neg = labels!=l; pos = labels==l\n",
    "\n",
    "            if sum(pos)==1: # if there is only one example in the mini-batch from a class, drop that class\n",
    "                continue\n",
    "            \n",
    "            anchors.append(i)\n",
    "            pos[i] = 0\n",
    "            p      = np.random.choice(np.where(pos)[0])\n",
    "            positives.append(p)\n",
    "\n",
    "            #Find negatives that violate tripet constraint semi-negatives\n",
    "            neg_mask = np.logical_and(neg,d>d[p])\n",
    "            neg_mask = np.logical_and(neg_mask,d<self.margin+d[p])\n",
    "            if neg_mask.sum()>0:\n",
    "                negatives.append(np.random.choice(np.where(neg_mask)[0]))\n",
    "            else:\n",
    "                negatives.append(np.random.choice(np.where(neg)[0]))\n",
    "\n",
    "        sampled_triplets = [[a, p, n] for a, p, n in zip(anchors, positives, negatives)]\n",
    "\n",
    "        if return_distances:\n",
    "            return sampled_triplets, distances\n",
    "        else:\n",
    "            return sampled_triplets\n",
    "\n",
    "\n",
    "    def pdist(self, A):\n",
    "        prod = torch.mm(A, A.t())\n",
    "        norm = prod.diag().unsqueeze(1).expand_as(prod)\n",
    "        res = (norm + norm.t() - 2 * prod).clamp(min = 0)\n",
    "        return res.clamp(min = 0).sqrt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# main code starts here\n",
    "\n",
    "main_root = '../'\n",
    "data_dir=main_root+'data/'\n",
    "\n",
    "### Keep the following lines commented, unless triplet ids are provided in text files\n",
    "\n",
    "# # load training data picked by MoM\n",
    "# mined_data = read_mined_data(data_dir+'anchors.txt',data_dir+'pos.txt',data_dir+'neg.txt')#,data_dir+'posw.txt',data_dir+'negw.txt')\n",
    "\n",
    "# # print(mined_data.keys()) #dict_keys(['anchors', 'pospool', 'negpool', 'posweight', 'negweight'])\n",
    "# # #anchors is a dictionary with a single value for a key. Rest are dictionaries with a list of values for a key.\n",
    "# # for key in mined_data.keys():\n",
    "# #     print(key,type(mined_data[key]))\n",
    "# #     print(len(mined_data[key]),mined_data[key])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logs saved in =>  ../results/CUB_gumlLoss_d_128_a_45_log.txt\n"
     ]
    }
   ],
   "source": [
    "results_dir = main_root+'results/' \n",
    "\n",
    "log_filename = results_dir+dataset_name+'_'+loss_name+'_d_'+str(opts.emb_dim)+'_a_'+str(opts.alpha)+'_'+'log.txt'\n",
    "print('logs saved in => ',log_filename)\n",
    "\n",
    "log = open(log_filename, 'a')\n",
    "\n",
    "# set random seeds\n",
    "for set_random_seed in [random.seed, np.random.seed, torch.manual_seed]: set_random_seed(8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inception_v1_googlenet(\n",
      "  (conv1): Sequential(\n",
      "    (7x7_s2): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3))\n",
      "    (relu1): ReLU(inplace=True)\n",
      "    (pool1): MaxPool2d(kernel_size=(3, 3), stride=(2, 2), padding=0, dilation=1, ceil_mode=True)\n",
      "    (lrn1): CrossMapLRN2d(5, alpha=0.0001, beta=0.75, k=1)\n",
      "  )\n",
      "  (conv2): Sequential(\n",
      "    (3x3_reduce): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (relu1): ReLU(inplace=True)\n",
      "    (3x3): Conv2d(64, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (relu2): ReLU(inplace=True)\n",
      "    (lrn2): CrossMapLRN2d(5, alpha=0.0001, beta=0.75, k=1)\n",
      "    (pool2): MaxPool2d(kernel_size=(3, 3), stride=(2, 2), padding=0, dilation=1, ceil_mode=True)\n",
      "  )\n",
      "  (inception_3a): InceptionModule(\n",
      "    (1x1): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (1x1_relu): ReLU(inplace=True)\n",
      "    (3x3_reduce): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (3x3_relu1): ReLU(inplace=True)\n",
      "    (3x3): Conv2d(96, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3x3_relu2): ReLU(inplace=True)\n",
      "    (5x5_reduce): Conv2d(192, 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (5x5_relu1): ReLU(inplace=True)\n",
      "    (5x5): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (5x5_relu2): ReLU(inplace=True)\n",
      "    (pool_pool): MaxPool2d(kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), dilation=1, ceil_mode=False)\n",
      "    (pool_proj): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (pool_relu): ReLU(inplace=True)\n",
      "  )\n",
      "  (inception_3b): InceptionModule(\n",
      "    (1x1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (1x1_relu): ReLU(inplace=True)\n",
      "    (3x3_reduce): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (3x3_relu1): ReLU(inplace=True)\n",
      "    (3x3): Conv2d(128, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3x3_relu2): ReLU(inplace=True)\n",
      "    (5x5_reduce): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (5x5_relu1): ReLU(inplace=True)\n",
      "    (5x5): Conv2d(32, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (5x5_relu2): ReLU(inplace=True)\n",
      "    (pool_pool): MaxPool2d(kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), dilation=1, ceil_mode=False)\n",
      "    (pool_proj): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (pool_relu): ReLU(inplace=True)\n",
      "  )\n",
      "  (pool3): MaxPool2d(kernel_size=(3, 3), stride=(2, 2), padding=0, dilation=1, ceil_mode=True)\n",
      "  (inception_4a): InceptionModule(\n",
      "    (1x1): Conv2d(480, 192, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (1x1_relu): ReLU(inplace=True)\n",
      "    (3x3_reduce): Conv2d(480, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (3x3_relu1): ReLU(inplace=True)\n",
      "    (3x3): Conv2d(96, 208, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3x3_relu2): ReLU(inplace=True)\n",
      "    (5x5_reduce): Conv2d(480, 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (5x5_relu1): ReLU(inplace=True)\n",
      "    (5x5): Conv2d(16, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (5x5_relu2): ReLU(inplace=True)\n",
      "    (pool_pool): MaxPool2d(kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), dilation=1, ceil_mode=False)\n",
      "    (pool_proj): Conv2d(480, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (pool_relu): ReLU(inplace=True)\n",
      "  )\n",
      "  (inception_4b): InceptionModule(\n",
      "    (1x1): Conv2d(512, 160, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (1x1_relu): ReLU(inplace=True)\n",
      "    (3x3_reduce): Conv2d(512, 112, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (3x3_relu1): ReLU(inplace=True)\n",
      "    (3x3): Conv2d(112, 224, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3x3_relu2): ReLU(inplace=True)\n",
      "    (5x5_reduce): Conv2d(512, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (5x5_relu1): ReLU(inplace=True)\n",
      "    (5x5): Conv2d(24, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (5x5_relu2): ReLU(inplace=True)\n",
      "    (pool_pool): MaxPool2d(kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), dilation=1, ceil_mode=False)\n",
      "    (pool_proj): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (pool_relu): ReLU(inplace=True)\n",
      "  )\n",
      "  (inception_4c): InceptionModule(\n",
      "    (1x1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (1x1_relu): ReLU(inplace=True)\n",
      "    (3x3_reduce): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (3x3_relu1): ReLU(inplace=True)\n",
      "    (3x3): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3x3_relu2): ReLU(inplace=True)\n",
      "    (5x5_reduce): Conv2d(512, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (5x5_relu1): ReLU(inplace=True)\n",
      "    (5x5): Conv2d(24, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (5x5_relu2): ReLU(inplace=True)\n",
      "    (pool_pool): MaxPool2d(kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), dilation=1, ceil_mode=False)\n",
      "    (pool_proj): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (pool_relu): ReLU(inplace=True)\n",
      "  )\n",
      "  (inception_4d): InceptionModule(\n",
      "    (1x1): Conv2d(512, 112, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (1x1_relu): ReLU(inplace=True)\n",
      "    (3x3_reduce): Conv2d(512, 144, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (3x3_relu1): ReLU(inplace=True)\n",
      "    (3x3): Conv2d(144, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3x3_relu2): ReLU(inplace=True)\n",
      "    (5x5_reduce): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (5x5_relu1): ReLU(inplace=True)\n",
      "    (5x5): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (5x5_relu2): ReLU(inplace=True)\n",
      "    (pool_pool): MaxPool2d(kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), dilation=1, ceil_mode=False)\n",
      "    (pool_proj): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (pool_relu): ReLU(inplace=True)\n",
      "  )\n",
      "  (inception_4e): InceptionModule(\n",
      "    (1x1): Conv2d(528, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (1x1_relu): ReLU(inplace=True)\n",
      "    (3x3_reduce): Conv2d(528, 160, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (3x3_relu1): ReLU(inplace=True)\n",
      "    (3x3): Conv2d(160, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3x3_relu2): ReLU(inplace=True)\n",
      "    (5x5_reduce): Conv2d(528, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (5x5_relu1): ReLU(inplace=True)\n",
      "    (5x5): Conv2d(32, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (5x5_relu2): ReLU(inplace=True)\n",
      "    (pool_pool): MaxPool2d(kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), dilation=1, ceil_mode=False)\n",
      "    (pool_proj): Conv2d(528, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (pool_relu): ReLU(inplace=True)\n",
      "  )\n",
      "  (pool4): MaxPool2d(kernel_size=(3, 3), stride=(2, 2), padding=0, dilation=1, ceil_mode=True)\n",
      "  (inception_5a): InceptionModule(\n",
      "    (1x1): Conv2d(832, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (1x1_relu): ReLU(inplace=True)\n",
      "    (3x3_reduce): Conv2d(832, 160, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (3x3_relu1): ReLU(inplace=True)\n",
      "    (3x3): Conv2d(160, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3x3_relu2): ReLU(inplace=True)\n",
      "    (5x5_reduce): Conv2d(832, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (5x5_relu1): ReLU(inplace=True)\n",
      "    (5x5): Conv2d(32, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (5x5_relu2): ReLU(inplace=True)\n",
      "    (pool_pool): MaxPool2d(kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), dilation=1, ceil_mode=False)\n",
      "    (pool_proj): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (pool_relu): ReLU(inplace=True)\n",
      "  )\n",
      "  (inception_5b): InceptionModule(\n",
      "    (1x1): Conv2d(832, 384, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (1x1_relu): ReLU(inplace=True)\n",
      "    (3x3_reduce): Conv2d(832, 192, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (3x3_relu1): ReLU(inplace=True)\n",
      "    (3x3): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3x3_relu2): ReLU(inplace=True)\n",
      "    (5x5_reduce): Conv2d(832, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (5x5_relu1): ReLU(inplace=True)\n",
      "    (5x5): Conv2d(48, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (5x5_relu2): ReLU(inplace=True)\n",
      "    (pool_pool): MaxPool2d(kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), dilation=1, ceil_mode=False)\n",
      "    (pool_proj): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (pool_relu): ReLU(inplace=True)\n",
      "  )\n",
      "  (pool5): AvgPool2d(kernel_size=(7, 7), stride=(1, 1), padding=0)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# load base model\n",
    "base_model = inception_v1_googlenet()\n",
    "base_model_weights_path = os.path.join(data_dir+'inception_v1_googlenet.h5') # 'googlenet.h5', 'inception_v1_googlenet.h5'\n",
    "\n",
    "temp=[]\n",
    "f = h5.File(base_model_weights_path, \"r\")\n",
    "gp_name=list(f.items())[0][0] #'data_0'\n",
    "G1=f.get(gp_name) # G1: Group 1 // is a <HDF5 group \"/data_0\" (114 members)>\n",
    "# G1_items=list(G1.items())\n",
    "\n",
    "with open('inception_v1_params_names.txt') as fp: \n",
    "    for line in fp:\n",
    "        #print(line.strip())\n",
    "        param_name=line.strip()\n",
    "        G1j=G1.get('/'+gp_name+'/'+param_name) \n",
    "        #G1j_items=list(G1j.items())\n",
    "        param=np.array(G1j.get('data_0'))\n",
    "        temp.append((param_name,param))\n",
    "#print(type(temp),temp)\n",
    "\n",
    "base_model.load_state_dict({k : torch.from_numpy(v) for k, v in temp})\n",
    "\n",
    "print(base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset found, and is proper!\n",
      "Dataset found, and is proper!\n"
     ]
    }
   ],
   "source": [
    "normalize = transforms.Compose([\n",
    " transforms.ToTensor(),\n",
    " transforms.Lambda(lambda x: x * base_model.rescale),\n",
    " transforms.Normalize(mean = base_model.rgb_mean, std = base_model.rgb_std),\n",
    " transforms.Lambda(lambda x: x[[2, 1, 0], ...])\n",
    "])\n",
    "\n",
    "dataset_train = CUB2011(train = True, transform = transforms.Compose([\n",
    " transforms.RandomResizedCrop(base_model.input_side),\n",
    " transforms.RandomHorizontalFlip(),\n",
    " normalize\n",
    "]), download = False) # Train data: Classes 1-100\n",
    "\n",
    "dataset_eval = CUB2011(train = False, transform = transforms.Compose([\n",
    " transforms.Resize(256),\n",
    " transforms.CenterCrop(base_model.input_side),\n",
    " normalize\n",
    "]), download = False) # Test data: Classes 101-200\n",
    "\n",
    "#print(dataset_train.num_training_classes,dataset_train.idx_to_class,dataset_train.imgs)\n",
    "#print(dataset_eval.num_training_classes,dataset_eval.idx_to_class,dataset_eval.imgs)\n",
    "#print(dataset_train.imgs)\n",
    "\n",
    "#print(type(dataset_train.imgs[0]),dataset_train.imgs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pseudo-labels assignment done.\n"
     ]
    }
   ],
   "source": [
    "main_root = '../'\n",
    "aas_labels_dir=main_root+'aas_labels_fgvc/'\n",
    "\n",
    "if dataset_name=='CUB':\n",
    "    aas_dataset='full_CUB_googlenet'\n",
    "elif dataset_name=='Cars':\n",
    "    aas_dataset='full_Cars_googlenet'\n",
    "else:\n",
    "    aas_dataset='SOP_googlenet'\n",
    "\n",
    "op_filename=aas_labels_dir+'aas_labels_'+aas_dataset+'.mat'\n",
    "annots = loadmat(op_filename)\n",
    "#print(annots.keys())\n",
    "\n",
    "pred_labels=annots['aas_labels']\n",
    "#pred_labels=pred_labels.reshape(len(pred_labels),)\n",
    "\n",
    "#print(len(pred_labels),len(np.unique(pred_labels)))\n",
    "\n",
    "for i,pseudolabel in enumerate(pred_labels):\n",
    "    #print(i,pseudolabel)\n",
    "    dataset_train.imgs[i]=(dataset_train.imgs[i][0], int(pseudolabel))\n",
    "\n",
    "# print(dataset_train.imgs)\n",
    "# print(type(dataset_train.imgs[0]),dataset_train.imgs[0])\n",
    "\n",
    "print('Pseudo-labels assignment done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model, optimizer and scheduler initialized.\n"
     ]
    }
   ],
   "source": [
    "# loss, optimizer, scheduler\n",
    "#model = WeightedTriplet(base_model, dataset_train.num_training_classes, lr =opts.lr, embedding_size = emb_dim).to(device)\n",
    "\n",
    "model=CNNModel(base_model, dataset_train.num_training_classes).to(device)\n",
    "\n",
    "#criterion = TripletLoss(opts.margin)\n",
    "criterion = gumlLoss(opts.alpha)\n",
    "\n",
    "# if only those parameters need to be updated for which requires_grad=True\n",
    "optimizer = torch.optim.SGD(filter(lambda p: p.requires_grad, model.parameters()), weight_decay = 5e-4, lr = opts.lr, momentum = 0.9, dampening = 0.9)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, **dict(step_size = opts.step_size, gamma = opts.gamma))\n",
    "\n",
    "print('Model, optimizer and scheduler initialized.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed for getting test embeddings: 10.979093551635742\n"
     ]
    }
   ],
   "source": [
    "# evaluate on test set for initial network\n",
    "\n",
    "t1=time.time()\n",
    "model.eval()\n",
    "embeddings_all, labels_all = get_dataset_embeddings(model,dataset_eval)\n",
    "print('Time elapsed for getting test embeddings:', time.time()-t1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> (5924, 1024) <class 'numpy.ndarray'> (5924,)\n"
     ]
    }
   ],
   "source": [
    "print(type(embeddings_all),embeddings_all.shape,type(labels_all),labels_all.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset_name=='SOP':\n",
    "    k_arr=[1,10,100]\n",
    "else:\n",
    "    k_arr=[1,2,4,8]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #model.eval() # set to true if embeddings are required\n",
    "# dataset_train.triplet_mode = False\n",
    "\n",
    "# loader_train = torch.utils.data.DataLoader(dataset_train, shuffle=True,\n",
    "#                                            num_workers = 8, batch_size = opts.batch, drop_last = True)\n",
    "# model.train()\n",
    "\n",
    "# #print(loader_train)\n",
    "\n",
    "# example_batch = next(iter(loader_train))\n",
    "\n",
    "# # model.train() tells your model that you are training the model. \n",
    "# # So effectively layers like dropout, batchnorm etc. which behave different on the train \n",
    "# # and test procedures know what is going on and hence can behave accordingly.\n",
    "# # You can call either model.eval() or model.train(mode=False) to tell that you are testing\n",
    "\n",
    "# images, pseudolabels = example_batch\n",
    "\n",
    "# # print(images.size(),pseudolabels.size(),pseudolabels)\n",
    "# # img = images[10,:,:,:].numpy().transpose((1, 2, 0)) # visualize a random image in the batch\n",
    "# # img = np.clip(img, 0, 1)\n",
    "# # print('After transforms:')\n",
    "# # plt.imshow(img)\n",
    "\n",
    "# emb_ims=model(images.to(device))\n",
    "# # print(emb_ims.size(), torch.sum(emb_ims*emb_ims, 1), pseudolabels)\n",
    "# print(emb_ims.size())\n",
    "\n",
    "# triplet_miner= SemiHardMiner(opts) \n",
    "# sampled_triplets = triplet_miner(emb_ims,pseudolabels) #contains indices of examples from a batch\n",
    "\n",
    "# print('#triplets sampled:', len(sampled_triplets) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# anc_ids=[]\n",
    "# pos_ids=[]\n",
    "# neg_ids=[]\n",
    "# for trip in sampled_triplets:\n",
    "#     anc_ids.append(trip[0])\n",
    "#     pos_ids.append(trip[1])\n",
    "#     neg_ids.append(trip[2])\n",
    "# # print(sampled_triplets)\n",
    "# # print(anc_ids)\n",
    "# # print(pos_ids)\n",
    "# # print(neg_ids)\n",
    "\n",
    "# embed_a=emb_ims[anc_ids,:]\n",
    "# embed_p=emb_ims[pos_ids,:]\n",
    "# embed_n=emb_ims[neg_ids,:]\n",
    "\n",
    "# #print(embed_a.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# margin=0.2\n",
    "\n",
    "# batch,labels=emb_ims,pseudolabels\n",
    "# if isinstance(labels, torch.Tensor): labels = labels.detach().numpy()\n",
    "# bs = batch.size(0)\n",
    "\n",
    "# print(bs,labels)\n",
    "# #Return distance matrix for all elements in batch (BSxBS)\n",
    "# distances = pdist(batch.detach()).detach().cpu().numpy()        \n",
    "    \n",
    "# print(distances.shape,distances) # bxb np array\n",
    "\n",
    "# positives, negatives = [], []\n",
    "# anchors = []\n",
    "# for i in range(bs):\n",
    "#     #print(i)\n",
    "#     l, d = labels[i], distances[i]\n",
    "#     neg = labels!=l; pos = labels==l\n",
    "    \n",
    "#     if sum(pos)==1: # if there is no more example in the mini-batch from the same class\n",
    "#         continue\n",
    "    \n",
    "#     anchors.append(i)\n",
    "#     pos[i] = 0\n",
    "#     p      = np.random.choice(np.where(pos)[0])\n",
    "#     positives.append(p)\n",
    "\n",
    "#     #Find negatives that violate tripet constraint semi-negatives\n",
    "#     neg_mask = np.logical_and(neg,d>d[p])\n",
    "#     neg_mask = np.logical_and(neg_mask,d<margin+d[p])\n",
    "#     if neg_mask.sum()>0:\n",
    "#         negatives.append(np.random.choice(np.where(neg_mask)[0]))\n",
    "#     else:\n",
    "#         negatives.append(np.random.choice(np.where(neg)[0]))\n",
    "\n",
    "# sampled_triplets = [[a, p, n] for a, p, n in zip(anchors, positives, negatives)]\n",
    "\n",
    "# print(len(sampled_triplets))\n",
    "# for trip in sampled_triplets:\n",
    "#     #print(trip)\n",
    "#     print(labels[trip[0]],labels[trip[1]],labels[trip[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_dim=base_model.output_size\n",
    "latent_dim=opts.emb_dim\n",
    "\n",
    "manifold = Product([Euclidean(orig_dim, latent_dim), Grassmann(orig_dim, latent_dim)]) #list or tuples\n",
    "#manifold = Product([Grassmann(orig_dim, latent_dim)]) #list or tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "@pymanopt.function.PyTorch\n",
    "def cost(R_tensor,L_tensor):\n",
    "\n",
    "    xi_avgs=0.5*(xi_ancs+xi_poss) #dxT, T:#triplets\n",
    "    #print('xi_avgs.size',xi_avgs.size())\n",
    "    #print('xi_avgs.requires_grad',xi_avgs.requires_grad)\n",
    "\n",
    "    num_triplets=xi_avgs.size(1)\n",
    "    #print(num_triplets)\n",
    "\n",
    "    \n",
    "    R_tensor=R_tensor.to(device)\n",
    "    RRT=R_tensor@(R_tensor.transpose(0,1))\n",
    "    #print(RRT.size(),RRT.dtype)\n",
    "    #print('type_R_tensor',type(R_tensor))\n",
    "\n",
    "\n",
    "    listA=xi_ancs.transpose(0,1)#.tolist()\n",
    "    listB=xi_poss.transpose(0,1)#.tolist()\n",
    "\n",
    "\n",
    "\n",
    "    #print(listA.dtype,type(listA))\n",
    "\n",
    "    exps_plus=list(map(lambda elA, elB:\n",
    "           torch.exp(-(elA@RRT@elB)), \n",
    "           listA.double(), listB.double()))\n",
    "    #exps_plus=list(map(lambda elt: np.asscalar(elt), exps_plus))\n",
    "    exps_plus=torch.tensor(exps_plus).reshape(1,len(exps_plus))\n",
    "\n",
    "    #print(exps_plus,exps_plus.size())\n",
    "\n",
    "    listA=xi_avgs.transpose(0,1)\n",
    "    listB=xi_negs.transpose(0,1)\n",
    "\n",
    "    exps_minus=list(map(lambda elA, elB:\n",
    "            torch.exp(-(elA@RRT@elB)),\n",
    "            listA.double(), listB.double()))\n",
    "    #exps_minus=list(map(lambda elt: np.asscalar(elt), exps_minus))\n",
    "    exps_minus=torch.tensor(exps_minus).reshape(1,len(exps_minus))\n",
    "\n",
    "    w_is_plus = 1/(1+ exps_plus ) # weights\n",
    "    w_is_minus = 1-1/(1+ exps_minus ) # weights\n",
    "\n",
    "    w_is=0.5*(w_is_plus+w_is_minus) \n",
    "    #print(w_is,w_is.size())    \n",
    "\n",
    "    L_tensor=L_tensor.to(device)\n",
    "    LT=L_tensor.transpose(0,1)\n",
    "\n",
    "    delZaZp=xi_ancs-xi_poss\n",
    "    MhZaZp = LT @ delZaZp.double()\n",
    "    d_aps=torch.sum(MhZaZp*MhZaZp,0)\n",
    "    d_aps=d_aps.reshape(1,len(d_aps)) # 1xT\n",
    "    #print(delZaZp.shape,MhZaZp.shape,d_aps.shape)\n",
    "\n",
    "    delZnZm=xi_negs-xi_avgs\n",
    "    MhZnZm = LT @ delZnZm.double()\n",
    "    d_nms=torch.sum(MhZnZm*MhZnZm,0)\n",
    "    d_nms=d_nms.reshape(1,len(d_nms)) # 1xT\n",
    "\n",
    "    tan_sq_alpha=(math.tan(math.radians(alpha)))**2 # for angular \n",
    "\n",
    "    z_is=d_aps-4*tan_sq_alpha*d_nms # metric losses\n",
    "\n",
    "    #print(z_is.shape)\n",
    "\n",
    "    m_is=torch.log(1+torch.exp(z_is))\n",
    "    w_is=w_is.to(device)\n",
    "    m_is=m_is.to(device)\n",
    "\n",
    "    f_is=-w_is*m_is\n",
    "    #print(f_is.shape)\n",
    "\n",
    "    loss_guml= (1/num_triplets)*torch.sum(torch.log(1+torch.exp(-f_is)))\n",
    "\n",
    "    #print(loss_guml)\n",
    "    return loss_guml.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha=opts.alpha\n",
    "problem = Problem(manifold=manifold, cost=cost, verbosity=0)#, egrad=egrad) # verbosity=0 for no o/p, 2 for most\n",
    "# (3) Instantiate a Pymanopt solver\n",
    "solver = ConjugateGradient( maxiter=5 , logverbosity=2) # logverbosity controls how much info is stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/rapid_data/dsfashionshare/miniconda/envs/rdml/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss epoch 0: 0.7077\n",
      "Time elapsed for training the epoch: 117.07206726074219\n",
      "NMI recall@1,2,4,8 epoch 0: 55.547269 46.758947 59.503716 72.062796 82.849425\n",
      "loss epoch 1: 0.7056\n",
      "Time elapsed for training the epoch: 115.61474084854126\n",
      "NMI recall@1,2,4,8 epoch 1: 55.141807 46.758947 59.773803 72.299123 82.950711\n",
      "loss epoch 2: 0.7045\n",
      "Time elapsed for training the epoch: 115.02356719970703\n",
      "NMI recall@1,2,4,8 epoch 2: 55.633242 47.029033 59.773803 72.535449 82.815665\n",
      "loss epoch 3: 0.7040\n",
      "Time elapsed for training the epoch: 114.21500301361084\n",
      "NMI recall@1,2,4,8 epoch 3: 55.978688 47.079676 59.925723 71.995276 82.815665\n",
      "loss epoch 4: 0.7033\n",
      "Time elapsed for training the epoch: 115.88131785392761\n",
      "NMI recall@1,2,4,8 epoch 4: 55.590070 47.079676 59.740043 72.079676 82.849425\n",
      "loss epoch 5: 0.7027\n",
      "Time elapsed for training the epoch: 114.33155798912048\n",
      "NMI recall@1,2,4,8 epoch 5: 55.762441 46.944633 60.077649 72.045916 82.461172\n",
      "loss epoch 6: 0.7024\n",
      "Time elapsed for training the epoch: 116.77263760566711\n",
      "NMI recall@1,2,4,8 epoch 6: 55.322363 46.944633 59.875083 72.062796 82.089806\n",
      "loss epoch 7: 0.7022\n",
      "Time elapsed for training the epoch: 115.34138917922974\n",
      "NMI recall@1,2,4,8 epoch 7: 55.479115 46.944633 59.453070 71.893990 82.140446\n",
      "loss epoch 8: 0.7018\n",
      "Time elapsed for training the epoch: 114.67424440383911\n",
      "NMI recall@1,2,4,8 epoch 8: 55.135536 46.590140 59.115463 71.860230 82.191086\n",
      "loss epoch 9: 0.7017\n",
      "Time elapsed for training the epoch: 114.66107702255249\n",
      "NMI recall@1,2,4,8 epoch 9: 55.567337 46.674544 59.149224 71.944630 82.174206\n",
      "loss epoch 10: 0.7013\n",
      "Time elapsed for training the epoch: 115.41245651245117\n",
      "loss epoch 11: 0.7012\n",
      "Time elapsed for training the epoch: 115.14364194869995\n",
      "loss epoch 12: 0.7009\n",
      "Time elapsed for training the epoch: 115.63981366157532\n",
      "loss epoch 13: 0.7008\n",
      "Time elapsed for training the epoch: 116.36159610748291\n",
      "loss epoch 14: 0.7007\n",
      "Time elapsed for training the epoch: 117.23731875419617\n",
      "NMI recall@1,2,4,8 epoch 14: 54.106506 45.239702 58.305198 70.982444 81.313300\n",
      "loss epoch 15: 0.7004\n",
      "Time elapsed for training the epoch: 116.24987435340881\n",
      "loss epoch 16: 0.7004\n",
      "Time elapsed for training the epoch: 114.25977277755737\n",
      "loss epoch 17: 0.7003\n",
      "Time elapsed for training the epoch: 116.61160850524902\n",
      "loss epoch 18: 0.7002\n",
      "Time elapsed for training the epoch: 115.46256971359253\n",
      "loss epoch 19: 0.7001\n",
      "Time elapsed for training the epoch: 114.67021059989929\n",
      "NMI recall@1,2,4,8 epoch 19: 53.447144 44.226873 57.410532 70.222825 79.878461\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(opts.epochs):\n",
    "    \n",
    "    t1=time.time()\n",
    "    \n",
    "    dataset_train.triplet_mode = False\n",
    "    loader_train = torch.utils.data.DataLoader(dataset_train, shuffle=True,\n",
    "                                           num_workers = 8, batch_size = opts.batch, drop_last = True)\n",
    "    model.train()\n",
    "    # batch train\n",
    "    scheduler.step()\n",
    "    loss_all = []\n",
    "    for batch_idx, batch in enumerate(loader_train):\n",
    "    #for batch_idx, batch in enumerate(loader_train if model.criterion is not None else []):\n",
    "        #a_images, p_images, n_images, p_w, n_w  = [torch.autograd.Variable(tensor.to(device)) for tensor in batch]\n",
    "        #images, labels  = [torch.autograd.Variable(tensor.to(device)) for tensor in batch]\n",
    "        images, labels  = batch\n",
    "        \n",
    "        # // Obtain embeddings for images, and perform semi-hard mining on the batch\n",
    "        emb_ims=model(images.to(device))\n",
    "        triplet_miner= SemiHardMiner(opts) \n",
    "        sampled_triplets = triplet_miner(emb_ims,labels) #contains indices of examples from a batch\n",
    "        #print('#triplets sampled:', len(sampled_triplets) )\n",
    "        \n",
    "        if len(sampled_triplets)<1: # to ensure that the code does not break in case of no triplets\n",
    "            continue # proceed to the next mini-batch\n",
    "        \n",
    "        #embed1, embed2, embed3 : anc, pos, neg tensors of shape (T,d). T:#triplets, d:embedding size\n",
    "        anc_ids, pos_ids, neg_ids = [], [], []\n",
    "        for trip in sampled_triplets:\n",
    "            anc_ids.append(trip[0])\n",
    "            pos_ids.append(trip[1])\n",
    "            neg_ids.append(trip[2])\n",
    "\n",
    "        embed1=emb_ims[anc_ids,:].to(device) # anchor tensors (T,d): T: #triplets, d:orig_dim\n",
    "        embed2=emb_ims[pos_ids,:].to(device) # positive tensors\n",
    "        embed3=emb_ims[neg_ids,:].to(device) # negative tensors\n",
    "    \n",
    "        ## Above embedding tensors are already l2 normalized\n",
    "        \n",
    "        xi_ancs=embed1.transpose(0,1).detach() # dxb tensors, b=T i.e. #triplets in mini-batch\n",
    "        xi_poss=embed2.transpose(0,1).detach() # dxb\n",
    "        xi_negs=embed3.transpose(0,1).detach() # dxb        \n",
    "        \n",
    "        ##>>>>>>>>>>>>>>>> Fix the remaining network, and learn (R,L) <<<<<<<<<<<<<<<<#\n",
    "        ##>>>>>>>>>>>>>>>> learn (R,L) using Riemannian optimization (eg, RCGD) <<<<<<<<<<<<<<<<#\n",
    "        if (epoch==0 and batch_idx==0):#for the very first mini-batch\n",
    "            # let Pymanopt do the rest\n",
    "            xopt, optlog = solver.solve(problem) # start with a random point on manifold\n",
    "            R=xopt[0]\n",
    "            L=xopt[1] #dxl\n",
    "            #proj_matrix=L.T\n",
    "        else:\n",
    "            # let Pymanopt do the rest\n",
    "            xopt,optlog = solver.solve(problem, x=(R_old,L_old))\n",
    "            R=xopt[0]\n",
    "            L=xopt[1]\n",
    "            #proj_matrix=L.T\n",
    "            #print('L_type_shape',type(L),L.shape)\n",
    "\n",
    "        ## for the next mini-batch, start Riemannian optimization from the points where you leave now.   \n",
    "        R_old=R\n",
    "        L_old=L\n",
    "            \n",
    "        ##print(type(optlog['iterations']['f(x)']))        \n",
    "        ## img = torch.from_numpy(img).float().to(device) # transform numpy arrays to PyTorch tensors\n",
    "        \n",
    "        ##>>>>>>>>>>>>>>>> Compute Loss with forward pass using current estimates <<<<<<<<<<<<<<<<#\n",
    "        ## requires_grad=False by default. Therefore, won't get affected in backward pass.\n",
    "        R_tensor=torch.from_numpy(R).to(device)\n",
    "        L_tensor=torch.from_numpy(L).to(device)\n",
    "        loss=criterion(embed1, embed2, embed3, R_tensor,L_tensor)\n",
    "\n",
    "        ##>>>>>>>>>>>>>>>> Backpropagation <<<<<<<<<<<<<<<<#\n",
    "        ## Parameters learned using Riemannian optimization won't get affected\n",
    "        ## Rest of the network parameters will be updated (for eg, the last linear layer prior to L)\n",
    "#         print('P Before backward:', net.fc1[2].weight.data) # uncomment to see changes in updation\n",
    "        ## above remains same as obtained from previous mini-batch\n",
    "#         print('L Before backward:', L.T) # uncomment to see changes in updation\n",
    "        ## above gets changed as obtained from previous mini-batch, because of Riemmannian optimization\n",
    "        ## for the current mini-batch\n",
    "        \n",
    "        loss_all.append(loss.data.item())\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        ##>>>>>>>>>>>>>>>> Update parameters using learned gradients <<<<<<<<<<<<<<<<#\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "    print('loss epoch {}: {:.04f}'.format(epoch, np.mean(loss_all)))\n",
    "    log.write('loss epoch {}: {:.04f}\\n'.format(epoch, np.mean(loss_all)))\n",
    "    \n",
    "    print('Time elapsed for training the epoch:', time.time()-t1)\n",
    "    \n",
    "    \n",
    "    # evaluate on test set \n",
    "    if epoch < 10 or (epoch + 1) % 5 == 0 or (epoch + 1) == opts.epochs:\n",
    "        model.eval()\n",
    "        embeddings_all, labels_all = get_dataset_embeddings(model,dataset_eval)\n",
    "        \n",
    "        \n",
    "        rec = [recall(torch.Tensor(embeddings_all), torch.Tensor(list(labels_all)), x).item() for x in k_arr]\n",
    "\n",
    "        nmi = eval_nmi(torch.Tensor(embeddings_all), torch.Tensor(labels_all.reshape(len(labels_all),))) # provide Nxd data, (N,) labels\n",
    "\n",
    "        print('NMI recall@1,2,4,8 epoch {}: {:.06f} {:.06f} {:.06f} {:.06f} {:.06f}'.format(epoch, nmi*1e2, rec[0]*1e2, rec[1]*1e2, rec[2]*1e2, rec[3]*1e2))\n",
    "        log.write('NMI recall@1,2,4,8 epoch {}: {:.06f} {:.06f} {:.06f} {:.06f} {:.06f}'.format(epoch, nmi*1e2, rec[0]*1e2, rec[1]*1e2, rec[2]*1e2, rec[3]*1e2))\n",
    "\n",
    "# best_epoch=epoch\n",
    "# torch.save({'epoch': best_epoch + 1, 'state_dict': model.state_dict()},'{}/checkpoint_{}_{}.pth'.format(data_dir, best_epoch, dataset_name))        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rdml",
   "language": "python",
   "name": "rdml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
